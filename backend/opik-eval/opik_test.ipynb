{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ace2972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Opik is already configured. You can check the settings by viewing the config file at /Users/paddy/.opik.config\n",
      "OPIK: Configuration completed successfully. Traces will be logged to 'Default Project' project. To change the destination project, see: https://www.comet.com/docs/opik/tracing/log_traces#configuring-the-project-name\n",
      "OPIK: Started logging traces to the \"Default Project\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=019bc21c-14c2-7eb4-ae5d-ff03fabc6a68&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thinking:\n",
      "The user asks: \"What is likelihood and probability?\" They want an explanation of the concepts. We need to respond in a helpful manner, presumably as an assistant specialized in math and stats, as per system: \"You are a helpful assistant that only talks about Math and Stats.\" So we must stay on topic, giving definitions and explanation.\n",
      "\n",
      "We should discuss probability: measure of chance of an event, defined axiomatically (Kolmogorov) as a mapping from events to [0,1], obeying certain properties. Likelihood: function of parameters given data, often used in inference. Contrast between probability of data given parameters vs probability of parameters given data. Explanation of likelihood as a function of the parameter(s), not a probability distribution (though it can be normalized to yield posterior with Bayes). Also discuss maximum likelihood estimation (MLE). Possibly talk about different interpretations: frequentist vs Bayesian. Provide examples: coin toss, normal distribution.\n",
      "\n",
      "We should also mention the difference between joint probability, marginal, conditional, and likelihood being a conditional function of parameters given data: p(x|θ). Show how to compute. Perhaps talk about the relationship via Bayes theorem: posterior ∝ likelihood × prior.\n",
      "\n",
      "Additionally, mention that likelihood is not a probability distribution over parameters unless normalized (it doesn't integrate to 1 over parameter space generally). Might discuss that it can be used to compute likelihood ratio tests.\n",
      "\n",
      "Maybe also discuss \"likelihood principle\" and its implications.\n",
      "\n",
      "Answer can be structured:\n",
      "\n",
      "- Introduction\n",
      "- Probability\n",
      "   - Formal definition, axioms\n",
      "   - Frequentist, subjective, etc.\n",
      "   - Joint, marginal, conditional\n",
      "- Likelihood\n",
      "   - Definition as p(x|θ)\n",
      "   - Function of θ for fixed data\n",
      "   - Not a probability distribution\n",
      "   - Example: coin toss, normal distribution.\n",
      "   - Use in inference: MLE, likelihood ratio, Bayes.\n",
      "   - Likelihood principle\n",
      "- Relationship via Bayes theorem.\n",
      "- Practical tips: computing, log-likelihood, etc.\n",
      "- Summary.\n",
      "\n",
      "We should keep it within the constraints of talking about math and stats, no other content.\n",
      "\n",
      "Make sure to be thorough, accessible but accurate. Use definitions, formulas, maybe small example.\n",
      "\n",
      "I should also keep language simple, but can incorporate technical details.\n",
      "\n",
      "Given that the user likely wants a conceptual explanation, but can be also looking for differences. So answer accordingly.\n",
      "\n",
      "We must not mention we are a language model or any policy stuff. Just answer.\n",
      "\n",
      "Potential pitfalls: mention that the likelihood function is not a probability distribution, but it's used for inference.\n",
      "\n",
      "We can also mention that there are two types of interpretations: frequentist probability and Bayesian.\n",
      "\n",
      "Potential mention of \"likelihood function\" as a function that for each parameter value gives the probability (density) of the observed data under that parameter. It's a product of probabilities (or densities) of individual observations.\n",
      "\n",
      "If needed, include formulas: L(θ|x) = p(x|θ) = ∏_{i} p(x_i|θ) for independent data.\n",
      "\n",
      "Also discuss log-likelihood: ℓ(θ) = log L(θ|x) = Σ log p(x_i|θ). This is often used because it's easier.\n",
      "\n",
      "Add examples:\n",
      "\n",
      "- Coin toss: data: 7 heads out of 10 flips. Probability: P(k|H) = C(10,k) p^k (1-p)^(10-k). Likelihood: treat as function of p: L(p) = C(10,7) p^7 (1-p)^3. We can maximize this.\n",
      "\n",
      "- Normal: data X_i ~ N(μ, σ^2). Likelihood: L(μ,σ) = ∏ (1/(σ√(2π))) exp(-(x_i-μ)^2/(2σ^2)). Maximize gives MLE: μ̂ = sample mean, σ̂ = sqrt(sample variance * (n-1)/n) (or unbiased).\n",
      "\n",
      "Also mention that maximum likelihood can be used even for non-i.i.d. but with assumptions.\n",
      "\n",
      "Explain that the likelihood is central to many statistical methods: hypothesis testing via likelihood ratio test, information criteria, etc.\n",
      "\n",
      "Thus answer.\n",
      "\n",
      "Now write final answer in a clear, structured manner, with headings.\n",
      "\n",
      "Also consider maybe note that probability can be interpreted as long-run frequency or degree of belief, but here focusing on mathematical.\n",
      "\n",
      "We can also talk about the \"likelihood principle\" that states that two experiments with same likelihood function produce same inference.\n",
      "\n",
      "Add in small caution: be careful not to treat likelihood as a probability for the parameter.\n",
      "\n",
      "Ok, let's produce final answer.\n",
      "\n",
      "Text:\n",
      "## 1.  Probability – the mathematical notion of chance  \n",
      "\n",
      "**Definition (Kolmogorov axioms).**  \n",
      "Let \\(\\mathcal{F}\\) be a σ‑algebra of events in a sample space \\(\\Omega\\). A **probability measure** \\(P:\\mathcal{F}\\to[0,1]\\) satisfies  \n",
      "\n",
      "1. \\(P(\\Omega)=1\\)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import anthropic\n",
    "from opik import configure \n",
    "from opik.integrations.anthropic import track_anthropic \n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "configure() \n",
    "\n",
    "anthropic_client = anthropic.Anthropic()\n",
    "anthropic_client = track_anthropic(anthropic_client) \n",
    "message = anthropic_client.messages.create(\n",
    "    model=\"MiniMax-M2.1\",\n",
    "    max_tokens=1000,\n",
    "    system=\"You are a helpful assistant that only talks about Math and Stats.\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"What is linear algebra?\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "for block in message.content:\n",
    "    if block.type == \"thinking\":\n",
    "        print(f\"Thinking:\\n{block.thinking}\\n\")\n",
    "    elif block.type == \"text\":\n",
    "        print(f\"Text:\\n{block.text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa1c1bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FretCoach",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
