# FretCoach Opik Integration

## Overview

Opik provides observability and evaluation for FretCoach's AI features:
- AI Practice Recommendations
- Live Coaching Feedback
- AI Chat Coach

We needed to know if our AI actually helps users learn guitar better, so we integrated Opik to track, measure, and improve AI quality systematically.

## Implementation

### 1. Enhanced Tracing

**What we added:**
- Token counting and cost tracking for all LLM calls
- User context in traces (skill level, session history, performance metrics)
- Tool execution monitoring (database queries with timing and results)

**Why:**
- See which features are expensive ($0.0002 per recommendation)
- Filter traces by skill level to debug personalization
- Track SQL query performance and catch slow operations

**Example trace metadata:**
```json
{
  "user": "intermediate (12 sessions, avg 68% performance)",
  "weakest_area": "timing",
  "cost": "$0.00018 USD",
  "tokens": "150 input, 45 output",
  "duration": "1.2s"
}
```

### 2. Evaluation Framework

**What we built:**
- Production datasets: 85 coaching scenarios, 15 recommendation‚Üíoutcome pairs
- Custom metrics:
  - `CoachingHelpfulness` (LLM-as-Judge) - evaluates feedback quality
  - `RecommendationAccuracy` (heuristic) - measures improvement after following advice
  - `LearningProgressRate` (heuristic) - tracks user progress over time
  - `SafetyModeration` (API) - content safety checks
- Experiment pipeline to compare different prompts with quantified scores

**Why:**
- Move from "this looks good" to "scored 0.85 on helpfulness across 85 examples"
- Measure if recommendations actually help users improve
- Test prompt changes systematically instead of manually

**Example evaluation:**
```
Student: Intermediate, Pitch 62%, Scale 51%, Timing 48%
Feedback: "Good pitch at 62%! But timing at 48% needs work - try metronome at 60 BPM."

LLM-as-Judge Score: 0.85/1.00
‚úì Identified weakness correctly
‚úì Actionable advice (specific BPM)
‚úó Could be more encouraging
‚úì Appropriate for skill level
```

### 3. Agent Graph Visualization

LangGraph workflow visualization enabled in `web/web-backend/langgraph_workflow.py` - shows how the AI decides which tools to use and helps debug conversation flow.

### 4. Agent Optimization

**What we built:**
- Systematic prompt testing pipeline with 5 coaching prompt variants
- Automated evaluation using Opik's evaluate_prompt API
- Quantified improvement measurement across 85 production examples

**Prompt variants tested:**
- V1: Current production baseline (direct, conversational)
- V2: Structured metrics with explicit technique guidance
- V3: Learning science principles (deliberate practice)
- V4: Concise direct (maximum brevity)
- V5: Skill-level adaptive (adjusts to beginner/intermediate/advanced)

**Why:**
- Stop guessing which prompts work better
- Quantify improvements before deploying changes
- Test systematically on real production data instead of intuition

**Results:**
```
üèÜ Best Performer: V2 (Structured Metrics)
Score: 0.8650 vs baseline 0.8300
Improvement: +4.2%

All Variants Ranked:
1. V2 - Structured Metrics:    0.8650 (+4.2%)
2. V3 - Learning Science:      0.8575 (+3.3%)
3. V4 - Concise Direct:        0.8375 (+0.9%)
4. V1 - Current Production:    0.8300 (baseline)
```

**Key finding:** Explicit metric interpretations and technique-specific guidance improved feedback quality measurably across all test cases.

### 5. Production Monitoring

**What we built:**
- ProductionAutoScorer for real-time quality monitoring
- 3 production dashboards (Learning Progress, AI Quality, System Health)
- 4 online evaluation rules with smart sampling
- Automatic feedback score recording to Opik

**Dashboards configured:**
- **Learning Progress** (5 widgets): Performance trends, skill distribution, improvement rates, plan completion
- **AI Quality** (5 widgets): Helpfulness over time, recommendation accuracy, score distribution, quality by skill level
- **System Health** (6 widgets): Trace volume, error rate, daily cost, token usage, LLM latency

**Online evaluation rules:**
- Auto-score Coaching Helpfulness (10% sampling) - LLM-as-Judge quality scoring
- Safety & Moderation Check (5% sampling) - Content safety verification
- Response Relevance Check (5% sampling) - Context relevance validation
- Cost Anomaly Detection (100% sampling) - Heuristic cost monitoring

**Why:**
- Monitor production quality in real-time without manual checking
- Track user learning progress automatically
- Catch quality regressions before users complain
- Optimize costs by identifying anomalies

**Integration:**
Auto-scoring integrated directly in `live_coach_service.py`:
```python
if AUTO_SCORING_ENABLED and auto_scorer:
    score_result = auto_scorer.score_coaching_feedback(
        input_data=input_data,
        output=feedback,
        trace_id=session_id
    )
    # Scores 10% of production traces automatically
```

**Results:**
- 10% of production coaching interactions auto-scored
- Feedback scores visible in Opik dashboard
- Non-blocking (doesn't slow down responses)
- Error-tolerant (wrapped in try/except)

## Results

**Visibility:**
- 100% AI call coverage with rich metadata
- 85+ production examples captured

**Measurability:**
- Coaching helpfulness: 0.83-0.87 range (after optimization)
- Cost tracking: $0.0001-0.0002 per interaction
- Can run 85-example experiments in ~3 minutes
- Quantified 4.2% improvement through systematic testing

**Use Cases:**

*Debugging:* Filter traces by user_id to see why someone got unexpected recommendations - often reveals correct classification (e.g., 15 sessions but still beginner-level performance).

*Testing improvements:* Tested 5 coaching prompt variants on 20 real examples. V2 (Structured Metrics) won with 0.8650 vs baseline 0.8300 - deployed with data-backed confidence.

*Optimization pipeline:* Run experiments in 10 minutes (baseline) or 30 minutes (full). Automatically rank variants and generate deployment code.

*Production monitoring:* Auto-score 10% of coaching interactions for quality. Dashboards show learning progress, AI quality, and system health in real-time.

*Cost optimization:* Identified 4,877 tokens per response, implemented two-tier prompting, achieved 92% reduction. Cost anomaly detection flags traces over $0.02.

## Technical Details

**Files:**
- `backend/core/llm_utils.py` - Cost tracking utilities
- `backend/api/services/ai_agent_service.py` - Enhanced tracing
- `backend/api/services/live_coach_service.py` - Enhanced tracing
- `evaluation/custom_metrics.py` - Custom domain metrics
- `evaluation/create_datasets.py` - Dataset generation
- `evaluation/run_experiments.py` - Experiment runner
- `evaluation/optimize_prompts.py` - Agent optimization pipeline
- `evaluation/run_baseline_experiments.py` - Quick baseline testing
- `evaluation/production_monitoring.py` - Auto-scoring and monitoring setup

**Code patterns:**
```python
# Cost tracking
llm_metadata = track_llm_call(prompt, response, "gpt-4o-mini")
opik_context.update_current_trace(metadata=llm_metadata)

# User context in traces
OpikTracer(tags=["ai-mode", f"skill-{skill_level}"], metadata=user_context)

# Custom metrics
metric = CoachingHelpfulness(model="gpt-4o-mini")
result = metric.score(input={...}, output="feedback text")

# Experiments
evaluate(dataset, task_fn, scoring_metrics=[...], experiment_name="Test V2")

# Prompt optimization
from opik.evaluation import evaluate_prompt
result = evaluate_prompt(
    dataset=dataset,
    messages=prompt_variant,
    model="gpt-4o-mini",
    scoring_metrics=[CoachingHelpfulness()],
    experiment_name="V2 Test"
)

# Production auto-scoring
from evaluation.production_monitoring import ProductionAutoScorer
auto_scorer = ProductionAutoScorer(sampling_rate=0.1)
score_result = auto_scorer.score_coaching_feedback(
    input_data={...},
    output="feedback text",
    trace_id="session-123"
)
```

## Summary

| Before | After |
|--------|-------|
| "Is our AI helpful?" | "Scores 0.83-0.87 on helpfulness" |
| "Which prompt is better?" | "V2 scored 0.8650, V1 scored 0.8300 - deploy V2" |
| "What does this cost?" | "$0.0002 per recommendation" |
| "Why did this fail?" | "SQL query took 2.3s, here's the trace" |
| "Should we deploy this change?" | "Tested on 85 examples, +4.2% improvement - yes" |
| "How's production quality?" | "10% auto-scored, avg 0.85 helpfulness, no anomalies" |

Opik transformed FretCoach from guessing about AI quality to measuring and improving it with data.
