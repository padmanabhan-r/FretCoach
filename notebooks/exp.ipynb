{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357c8423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"/Users/paddy/Documents/Github/FretCoach/web/web-backend/.env\")\n",
    "\n",
    "from opik import Opik\n",
    "from opik.evaluation import evaluate\n",
    "from opik.evaluation.metrics import base_metric, score_result\n",
    "from typing import Any\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize Opik client\n",
    "client = Opik()\n",
    "\n",
    "# Get validation dataset\n",
    "dataset = client.get_dataset(name=\"fretcoach_live_ai_feedback_val_9\")\n",
    "\n",
    "# Initialize LLM for generating coaching feedback\n",
    "llm_client = OpenAI()\n",
    "\n",
    "# System prompt from live_coach_service.py (optimized version)\n",
    "COACHING_SYSTEM_PROMPT = \"\"\"You are a direct guitar coach giving quick real-time feedback. Your feedback MUST be 1-2 sentences, maximum 30 words total.\n",
    "\n",
    "Format: \"[What's good], but [what's weak] - [specific actionable fix based on performance context]\"\n",
    "\n",
    "To improve insight relevance, always relate feedback to specific performance scores, especially scores above 0.700. Include contextual details from the player's playing style to inform suggestions:\n",
    "\n",
    "- Pitch Accuracy: How cleanly notes are fretted (low = finger pressure issues)\n",
    "  → Fix: \"ease finger pressure to improve note clarity\" or \"focus on clean fretting by adjusting finger placement\"\n",
    "\n",
    "- Scale Conformity: Playing correct scale notes across fretboard positions (low = stuck in one position or wrong notes)\n",
    "  → Fix: \"explore positions 5-7 to enhance versatility\" or \"move up the fretboard to discover new notes\"\n",
    "\n",
    "- Timing Stability: Consistency of note spacing (low = rushing, dragging, uneven rhythm)\n",
    "  → Fix: \"use a metronome at 60 BPM to develop timing\" or \"slow down and count to create consistent spacing\"\n",
    "\n",
    "Be direct and conversational, and vary your wording. Ensure your suggestions are anchored in the player's specific performance metrics and informed by previous high-quality outputs. Maximum 30 words.\"\"\"\n",
    "\n",
    "# Initialize Perplexity client for LLM judge\n",
    "perplexity_client = OpenAI(\n",
    "    api_key=os.getenv(\"PPLX_API_KEY\"),\n",
    "    base_url=\"https://api.perplexity.ai\"\n",
    ")\n",
    "\n",
    "class CoachingQualityMetric(base_metric.BaseMetric):\n",
    "    \"\"\"LLM judge metric evaluating coaching feedback quality\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"coaching_quality\")\n",
    "    \n",
    "    def score(self, output: str, reference: str, input: str, **kwargs) -> score_result.ScoreResult:\n",
    "        \"\"\"\n",
    "        Evaluate coaching feedback quality\n",
    "        \n",
    "        Args:\n",
    "            output: Generated coaching feedback\n",
    "            reference: Expected coaching feedback\n",
    "            input: Input metrics (already shown in UI)\n",
    "        \"\"\"\n",
    "        judge_prompt = f\"\"\"You are evaluating guitar coaching feedback quality.\n",
    "\n",
    "INPUT METRICS (already shown in UI):\n",
    "{input}\n",
    "\n",
    "GENERATED FEEDBACK:\n",
    "{output}\n",
    "\n",
    "EXPECTED FEEDBACK EXAMPLE:\n",
    "{reference}\n",
    "\n",
    "Evaluate the GENERATED FEEDBACK on a scale of 0.0 to 1.0 based on:\n",
    "\n",
    "1. **Non-redundancy (40%)**: Does it avoid repeating metric numbers already in the UI?\n",
    "2. **Actionability (30%)**: Does it give specific, actionable advice?\n",
    "3. **Coaching value (30%)**: Does it interpret what the metrics mean and suggest concrete fixes?\n",
    "\n",
    "CRITICAL: Heavy penalty if feedback just restates numbers from the input.\n",
    "\n",
    "Respond with ONLY a number between 0.0 and 1.0, nothing else.\"\"\"\n",
    "\n",
    "        response = perplexity_client.chat.completions.create(\n",
    "            model=\"sonar-pro\",\n",
    "            messages=[{\"role\": \"user\", \"content\": judge_prompt}],\n",
    "            max_tokens=10,\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        score_text = response.choices[0].message.content.strip()\n",
    "        try:\n",
    "            score_value = float(score_text)\n",
    "            score_value = max(0.0, min(1.0, score_value))\n",
    "        except ValueError:\n",
    "            score_value = 0.0\n",
    "        \n",
    "        return score_result.ScoreResult(\n",
    "            value=score_value,\n",
    "            name=self.name,\n",
    "            reason=f\"LLM judge score: {score_value:.3f} (non-redundancy + actionability)\",\n",
    "        )\n",
    "\n",
    "def evaluation_task(dataset_item):\n",
    "    \"\"\"Generate coaching feedback for evaluation\"\"\"\n",
    "    # Get the input metrics\n",
    "    user_input = dataset_item['input']\n",
    "    \n",
    "    # Call LLM to generate coaching feedback\n",
    "    response = llm_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": COACHING_SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ],\n",
    "        temperature=0.9,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    \n",
    "    llm_response = response.choices[0].message.content.strip()\n",
    "    \n",
    "    result = {\n",
    "        \"input\": user_input,\n",
    "        \"output\": llm_response,\n",
    "        \"reference\": dataset_item['expected_output']\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Create metric instance\n",
    "metrics = [CoachingQualityMetric()]\n",
    "\n",
    "eval_results = evaluate(\n",
    "    experiment_name=\"fretcoach-coaching-feedback-eval\",\n",
    "    dataset=dataset,\n",
    "    task=evaluation_task,\n",
    "    scoring_metrics=metrics\n",
    ")\n",
    "\n",
    "\n",
    "import os\n",
    "from opik import Opik\n",
    "from opik.evaluation import evaluate\n",
    "\n",
    "\n",
    "from opik.evaluation.metrics import (Hallucination, LevenshteinRatio, Moderation, AnswerRelevance, ContextRecall, ContextPrecision)\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize clients\n",
    "client = Opik()\n",
    "llm_client = OpenAI()\n",
    "\n",
    "# Get validation dataset\n",
    "dataset = client.get_dataset(name=\"fretcoach_live_ai_feedback_val_9\")\n",
    "\n",
    "# System prompt from live_coach_service.py (optimized version)\n",
    "COACHING_SYSTEM_PROMPT = \"\"\"You are a direct guitar coach giving quick real-time feedback. Your feedback MUST be 1-2 sentences, maximum 30 words total.\n",
    "\n",
    "Format: \"[What's good], but [what's weak] - [specific actionable fix based on performance context]\"\n",
    "\n",
    "To improve insight relevance, always relate feedback to specific performance scores, especially scores above 0.700. Include contextual details from the player's playing style to inform suggestions:\n",
    "\n",
    "- Pitch Accuracy: How cleanly notes are fretted (low = finger pressure issues)\n",
    "  → Fix: \"ease finger pressure to improve note clarity\" or \"focus on clean fretting by adjusting finger placement\"\n",
    "\n",
    "- Scale Conformity: Playing correct scale notes across fretboard positions (low = stuck in one position or wrong notes)\n",
    "  → Fix: \"explore positions 5-7 to enhance versatility\" or \"move up the fretboard to discover new notes\"\n",
    "\n",
    "- Timing Stability: Consistency of note spacing (low = rushing, dragging, uneven rhythm)\n",
    "  → Fix: \"use a metronome at 60 BPM to develop timing\" or \"slow down and count to create consistent spacing\"\n",
    "\n",
    "Be direct and conversational, and vary your wording. Ensure your suggestions are anchored in the player's specific performance metrics and informed by previous high-quality outputs. Maximum 30 words.\"\"\"\n",
    "\n",
    "def evaluation_task_default_metrics(dataset_item):\n",
    "    \"\"\"Generate coaching feedback for evaluation with default metrics\"\"\"\n",
    "    # Get the input metrics\n",
    "    user_input = dataset_item['input']\n",
    "    \n",
    "    # Call LLM to generate coaching feedback\n",
    "    response = llm_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": COACHING_SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ],\n",
    "        temperature=0.9,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    \n",
    "    llm_response = response.choices[0].message.content.strip()\n",
    "    \n",
    "    result = {\n",
    "        \"input\": user_input,\n",
    "        \"output\": llm_response,\n",
    "        \"reference\": dataset_item['expected_output'],\n",
    "        \"context\": [COACHING_SYSTEM_PROMPT]  # System prompt as context\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Default Opik metrics\n",
    "metrics = [\n",
    "    Hallucination(),\n",
    "    LevenshteinRatio(),\n",
    "    Moderation(),\n",
    "    AnswerRelevance(),\n",
    "    ContextRecall(),\n",
    "    ContextPrecision()\n",
    "]\n",
    "\n",
    "eval_results_default = evaluate(\n",
    "    experiment_name=\"fretcoach-default-metrics-eval\",\n",
    "    dataset=dataset,\n",
    "    task=evaluation_task_default_metrics,\n",
    "    scoring_metrics=metrics\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
